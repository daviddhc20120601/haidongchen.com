---
title: "VLAæœºå™¨äººæ–°å‘å±•ï¼šä¸»æµè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å¯¹æ¯”ä¸å®æˆ˜æ•™ç¨‹"
collection: talks
type: "Technical Blog"
venue: "VLA Robotics Community"
location: "Online"
date: 2025-12-06
tags: ["VLA", "è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹", "æœºå™¨äººå­¦", "å…·èº«æ™ºèƒ½", "OpenVLA", "Gemini Robotics", "Figure Helix", "Pi0"]
excerpt: "æ·±å…¥æ¢ç´¢Vision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹çš„æœ€æ–°å‘å±•ï¼Œå¯¹æ¯”OpenVLAã€Gemini Robotics 1.5ã€Figure Helixã€Pi0ç­‰ä¸»æµæ¨¡å‹ï¼Œå¹¶æä¾›è¯¦ç»†çš„å®æˆ˜æ•™ç¨‹å¸®åŠ©ä½ å¿«é€Ÿä¸Šæ‰‹è¿è¡Œè¿™äº›å‰æ²¿æ¨¡å‹ã€‚"
---

# VLAæœºå™¨äººæ–°å‘å±•ï¼šä¸»æµè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å¯¹æ¯”ä¸å®æˆ˜æ•™ç¨‹

åœ¨å…·èº«æ™ºèƒ½ï¼ˆEmbodied AIï¼‰é¢†åŸŸï¼Œ**Vision-Language-Actionï¼ˆVLAï¼‰æ¨¡å‹**æ­£åœ¨å¼•é¢†ä¸€åœºé©å‘½æ€§çš„å˜é©ã€‚è¿™äº›æ¨¡å‹å°†è§†è§‰æ„ŸçŸ¥ã€è‡ªç„¶è¯­è¨€ç†è§£å’Œæœºå™¨äººåŠ¨ä½œç”Ÿæˆç»Ÿä¸€åˆ°ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ¡†æ¶ä¸­ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿé€šè¿‡ç®€å•çš„è¯­è¨€æŒ‡ä»¤æ‰§è¡Œå¤æ‚ä»»åŠ¡ã€‚æœ¬æ–‡å°†æ·±å…¥åˆ†æå½“å‰ä¸»æµVLAæ¨¡å‹çš„æœ€æ–°å‘å±•ï¼Œå¹¶æä¾›è¯¦ç»†çš„å®æˆ˜æ•™ç¨‹ã€‚

## ç›®å½•

1. [ä»€ä¹ˆæ˜¯VLAæ¨¡å‹ï¼Ÿ](#ä»€ä¹ˆæ˜¯vlaæ¨¡å‹)
2. [ä¸»æµVLAæ¨¡å‹å¯¹æ¯”](#ä¸»æµvlaæ¨¡å‹å¯¹æ¯”)
3. [æ·±åº¦è§£æï¼šå„æ¨¡å‹æŠ€æœ¯ç‰¹ç‚¹](#æ·±åº¦è§£æå„æ¨¡å‹æŠ€æœ¯ç‰¹ç‚¹)
4. [å®æˆ˜æ•™ç¨‹ï¼šå¿«é€Ÿä¸Šæ‰‹VLAæ¨¡å‹](#å®æˆ˜æ•™ç¨‹å¿«é€Ÿä¸Šæ‰‹vlaæ¨¡å‹)
5. [VLAæ¨¡å‹é¢ä¸´çš„åå¤§æŒ‘æˆ˜](#vlaæ¨¡å‹é¢ä¸´çš„åå¤§æŒ‘æˆ˜)
6. [æ€»ç»“ä¸å±•æœ›](#æ€»ç»“ä¸å±•æœ›)

---

## ä»€ä¹ˆæ˜¯VLAæ¨¡å‹ï¼Ÿ

VLAï¼ˆVision-Language-Actionï¼‰æ¨¡å‹æ˜¯ä¸€ç±»æ–°å…´çš„å¤šæ¨¡æ€AIç³»ç»Ÿï¼Œå®ƒä»¬èƒ½å¤Ÿï¼š

1. **è§†è§‰æ„ŸçŸ¥**ï¼šé€šè¿‡æ‘„åƒå¤´ç†è§£å‘¨å›´ç¯å¢ƒå’Œç‰©ä½“
2. **è¯­è¨€ç†è§£**ï¼šè§£æè‡ªç„¶è¯­è¨€æŒ‡ä»¤
3. **åŠ¨ä½œç”Ÿæˆ**ï¼šè¾“å‡ºæœºå™¨äººæ§åˆ¶å‘½ä»¤æ¥æ‰§è¡Œä»»åŠ¡

```mermaid
flowchart LR
    Camera[ğŸ“· è§†è§‰è¾“å…¥] --> VLAModel[VLAæ¨¡å‹]
    Language[ğŸ’¬ è¯­è¨€æŒ‡ä»¤] --> VLAModel
    VLAModel --> Action[ğŸ¦¾ æœºå™¨äººåŠ¨ä½œ]
    
    subgraph VLAæ¨¡å‹å†…éƒ¨
        direction TB
        VisionEncoder[è§†è§‰ç¼–ç å™¨] --> Fusion[å¤šæ¨¡æ€èåˆ]
        LanguageEncoder[è¯­è¨€ç¼–ç å™¨] --> Fusion
        Fusion --> ActionDecoder[åŠ¨ä½œè§£ç å™¨]
    end
```

ä¸ä¼ ç»Ÿçš„æ¨¡å—åŒ–æœºå™¨äººç³»ç»Ÿä¸åŒï¼ŒVLAæ¨¡å‹å®ç°äº†æ„ŸçŸ¥ã€ç†è§£ã€è§„åˆ’å’Œæ‰§è¡Œçš„ç«¯åˆ°ç«¯å­¦ä¹ ï¼Œå¤§å¤§ç®€åŒ–äº†æœºå™¨äººç³»ç»Ÿçš„å¤æ‚åº¦ã€‚

---

## ä¸»æµVLAæ¨¡å‹å¯¹æ¯”

ä»¥ä¸‹æ˜¯å½“å‰æœ€å…·å½±å“åŠ›çš„VLAæ¨¡å‹å¯¹æ¯”è¡¨ï¼š

| æ¨¡å‹ | å¼€å‘è€… | å‚æ•°é‡ | å¼€æºçŠ¶æ€ | ä¸»è¦ç‰¹ç‚¹ | ç¡¬ä»¶è¦æ±‚ |
|------|--------|--------|----------|----------|----------|
| **OpenVLA** | Stanford/Berkeley | 7B | âœ… å®Œå…¨å¼€æº | å¼€æºæ ‡æ†ï¼Œæ”¯æŒLoRAå¾®è°ƒ | RTX 3060+ (12GB+) |
| **Gemini Robotics 1.5** | Google DeepMind | æœªå…¬å¼€ | âš ï¸ APIè®¿é—® | å·¥å…·ä½¿ç”¨ã€æ¨ç†é€æ˜ | äº‘ç«¯API |
| **Figure Helix** | Figure AI | æœªå…¬å¼€ | âŒ é—­æº | å…¨èº«äººå½¢æ§åˆ¶ | æœºè½½åµŒå…¥å¼GPU |
| **Ï€â‚€ (Pi0)** | Physical Intelligence | 3BåŸºåº§ | âœ… å¼€æº | æµåŒ¹é…åŠ¨ä½œç”Ÿæˆ | RTX 4090+ |
| **RT-2-X** | Google DeepMind | 55B | âŒ é—­æº | WebçŸ¥è¯†è¿ç§» | äº‘ç«¯TPU |
| **Octo** | Berkeley | 27-93M | âœ… å¼€æº | æ‰©æ•£ç­–ç•¥ã€å°æ¨¡å‹ | RTX 2080+ |

### æ€§èƒ½å¯¹æ¯”ï¼ˆåŸºäºOpen X-EmbodimentåŸºå‡†ï¼‰

```mermaid
xychart-beta
    title "VLAæ¨¡å‹ä»»åŠ¡æˆåŠŸç‡å¯¹æ¯”"
    x-axis ["OpenVLA-7B", "RT-2-X (55B)", "Octo-Base", "Pi0"]
    y-axis "æˆåŠŸç‡ (%)" 0 --> 100
    bar [71.5, 55.0, 48.3, 68.2]
```

**å…³é”®å‘ç°**ï¼š
- OpenVLA-7Båœ¨29é¡¹ä»»åŠ¡ä¸­æ¯”RT-2-Xé«˜å‡º**16.5%**çš„æˆåŠŸç‡ï¼Œå°½ç®¡å‚æ•°é‡åªæœ‰åè€…çš„1/7
- å¼€æºæ¨¡å‹æ­£åœ¨èµ¶è¶…é—­æºå•†ä¸šæ¨¡å‹

---

## æ·±åº¦è§£æï¼šå„æ¨¡å‹æŠ€æœ¯ç‰¹ç‚¹

### 1. OpenVLA-7Bï¼šå¼€æºVLAçš„æ ‡æ†

![OpenVLAæ¶æ„](https://openvla.github.io/static/images/openvla_teaser.jpg)
*OpenVLAæ¨¡å‹æ¶æ„ç¤ºæ„*

**æ ¸å¿ƒæ¶æ„**ï¼š
- **è§†è§‰ç¼–ç å™¨**ï¼šDINOv2 + SigLIPåŒç¼–ç å™¨
- **è¯­è¨€éª¨å¹²**ï¼šLlama-2 7B
- **è®­ç»ƒæ•°æ®**ï¼šOpen X-Embodimentï¼ˆ97ä¸‡çœŸå®æœºå™¨äººæ¼”ç¤ºï¼‰

**æŠ€æœ¯äº®ç‚¹**ï¼š
```python
# OpenVLAé‡‡ç”¨åŠ¨ä½œæ ‡è®°åŒ–æ–¹æ³•
# å°†è¿ç»­åŠ¨ä½œç¦»æ•£åŒ–ä¸ºè¯­è¨€æ¨¡å‹å¯å¤„ç†çš„token
action_tokens = tokenize_actions(
    delta_position=[dx, dy, dz],      # ä½ç½®å˜åŒ–
    delta_rotation=[rx, ry, rz],      # æ—‹è½¬å˜åŒ–  
    gripper_action=open_or_close      # å¤¹çˆªåŠ¨ä½œ
)
```

### 2. Gemini Robotics 1.5ï¼šå°†AIä»£ç†å¸¦å…¥ç‰©ç†ä¸–ç•Œ

Google DeepMindçš„[Gemini Robotics 1.5](https://deepmind.google/blog/gemini-robotics-15-brings-ai-agents-into-the-physical-world/)ä»£è¡¨äº†VLAæ¨¡å‹çš„ä¸€ä¸ªé‡è¦é‡Œç¨‹ç¢‘ã€‚

**åŒæ¨¡å‹æ¶æ„**ï¼š
- **Gemini Robotics-ER 1.5**ï¼šé«˜çº§æ¨ç†å¼•æ“ï¼Œè´Ÿè´£è§„åˆ’å’Œå·¥å…·è°ƒç”¨
- **Gemini Robotics 1.5**ï¼šVLAæ‰§è¡Œæ¨¡å‹ï¼Œè´Ÿè´£å…·ä½“åŠ¨ä½œæ§åˆ¶

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant ER as Gemini Robotics-ER
    participant VLA as Gemini Robotics
    participant Robot as æœºå™¨äºº
    participant API as å¤–éƒ¨API

    User->>ER: "æ•´ç†è¿™äº›å›æ”¶ç‰©å“"
    ER->>API: æŸ¥è¯¢å½“åœ°å›æ”¶è§„åˆ™
    API-->>ER: è¿”å›åˆ†ç±»æŒ‡å—
    ER->>ER: è§„åˆ’ä»»åŠ¡æ­¥éª¤
    ER->>VLA: æ‰§è¡Œæ­¥éª¤1ï¼šæ‹¿èµ·å¡‘æ–™ç“¶
    VLA->>Robot: æ§åˆ¶æŒ‡ä»¤
    Robot-->>VLA: æ‰§è¡Œåé¦ˆ
    VLA-->>ER: æ­¥éª¤å®Œæˆ
    ER->>VLA: æ‰§è¡Œæ­¥éª¤2ï¼šæ”¾å…¥è“è‰²å›æ”¶æ¡¶
```

**åˆ›æ–°ç‰¹æ€§**ï¼š
- **é€æ˜æ¨ç†**ï¼šç”Ÿæˆå¯å®¡æŸ¥çš„"æ€è€ƒè½¨è¿¹"
- **å·¥å…·ä½¿ç”¨**ï¼šå¯è°ƒç”¨æœç´¢ã€APIç­‰å¤–éƒ¨å·¥å…·
- **å¤šæœºå™¨äººè¿ç§»**ï¼šä¸€æ¬¡è®­ç»ƒï¼Œå¤šç§æœºå™¨äººéƒ¨ç½²

### 3. Figure Helixï¼šäººå½¢æœºå™¨äººçš„é€šç”¨å¤§è„‘

[Figure AIçš„Helix](https://www.figure.ai/news/helix)æ˜¯ä¸“ä¸ºäººå½¢æœºå™¨äººè®¾è®¡çš„VLAæ¨¡å‹ã€‚

**çªç ´æ€§èƒ½åŠ›**ï¼š
- **å…¨èº«æ§åˆ¶**ï¼šå•ä¸€ç¥ç»ç½‘ç»œæ§åˆ¶æ‰‹è…•ã€æ‰‹æŒ‡ã€èº¯å¹²å’Œå¤´éƒ¨
- **é›¶æ ·æœ¬æ³›åŒ–**ï¼šæ— éœ€é’ˆå¯¹ç‰¹å®šç‰©ä½“è®­ç»ƒå³å¯æ“ä½œæ•°åƒç§ç‰©å“
- **å¤šæœºåä½œ**ï¼šä¸¤ä¸ªæœºå™¨äººå¯ååŒå®Œæˆé•¿horizonä»»åŠ¡

**è®­ç»ƒæ•°æ®å¤šæ ·æ€§**ï¼š
```mermaid
pie title Helixè®­ç»ƒæ•°æ®ç»„æˆ
    "é¥æ“ä½œæ•°æ®" : 35
    "åˆæˆè§†é¢‘" : 25
    "è‡ªåŠ¨æ ‡æ³¨æ•°æ®" : 25
    "å¼ºåŒ–å­¦ä¹ " : 15
```

### 4. Ï€â‚€ (Pi0)ï¼šç‰©ç†æ™ºèƒ½çš„å¼€æºå…ˆé”‹

[Physical Intelligence](https://www.physicalintelligence.company/blog/openpi)æ¨å‡ºçš„Ï€â‚€æ¨¡å‹é‡‡ç”¨äº†åˆ›æ–°çš„**æµåŒ¹é…ï¼ˆFlow Matchingï¼‰**æŠ€æœ¯ã€‚

**æŠ€æœ¯å·®å¼‚åŒ–**ï¼š
- **è¿ç»­åŠ¨ä½œæµ**ï¼šä¸åŒäºç¦»æ•£tokenï¼Œç›´æ¥ç”Ÿæˆè¿ç»­è½¨è¿¹
- **ç²¾ç»†æ“ä½œ**ï¼šæ›´é€‚åˆéœ€è¦é«˜ç²¾åº¦çš„ä»»åŠ¡
- **å¼€æºç”Ÿæ€**ï¼šå®Œæ•´çš„è®­ç»ƒå’Œæ¨ç†ä»£ç 

```python
# Pi0ä½¿ç”¨æµåŒ¹é…è¿›è¡ŒåŠ¨ä½œç”Ÿæˆ
# ç›¸æ¯”ä¼ ç»Ÿæ‰©æ•£æ¨¡å‹ï¼Œè®­ç»ƒæ›´ç¨³å®šï¼Œæ¨ç†æ›´å¿«
flow_trajectory = pi0.generate_trajectory(
    visual_observation=camera_image,
    language_instruction="å°†æ¯å­æ”¾åˆ°ç›˜å­ä¸Š",
    horizon=32  # é¢„æµ‹æœªæ¥32æ­¥åŠ¨ä½œ
)
```

---

## å®æˆ˜æ•™ç¨‹ï¼šå¿«é€Ÿä¸Šæ‰‹VLAæ¨¡å‹

### æ•™ç¨‹1ï¼šOpenVLAå¿«é€Ÿå…¥é—¨

#### ç³»ç»Ÿè¦æ±‚
- **GPU**ï¼šNVIDIA RTX 3060+ (12GB+ VRAM)
- **Python**ï¼š3.10+
- **OS**ï¼šUbuntu 22.04 / Windows 11 / macOS

#### å®‰è£…æ­¥éª¤

```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n openvla python=3.10 -y
conda activate openvla

# 2. å®‰è£…PyTorch (æ ¹æ®CUDAç‰ˆæœ¬é€‰æ‹©)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# 3. å…‹éš†å¹¶å®‰è£…OpenVLA
git clone https://github.com/openvla/openvla.git
cd openvla
pip install -e .

# 4. å®‰è£…Flash Attentionï¼ˆå¯é€‰ï¼Œæå‡é€Ÿåº¦ï¼‰
pip install flash-attn --no-build-isolation
```

#### è¿è¡Œæ¨ç†

```python
from transformers import AutoModelForVision2Seq, AutoProcessor
from PIL import Image
import torch

# åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
processor = AutoProcessor.from_pretrained(
    "openvla/openvla-7b", 
    trust_remote_code=True
)
model = AutoModelForVision2Seq.from_pretrained(
    "openvla/openvla-7b",
    attn_implementation="flash_attention_2",
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True
).to("cuda:0")

# åŠ è½½æœºå™¨äººç›¸æœºå›¾åƒ
image = Image.open("robot_camera.png")

# æ„å»ºæç¤ºè¯
prompt = "In: What action should the robot take to pick up the red block?\nOut:"

# é¢„å¤„ç†è¾“å…¥
inputs = processor(prompt, image).to("cuda:0", dtype=torch.bfloat16)

# é¢„æµ‹åŠ¨ä½œ (è¿”å›7-DoFåŠ¨ä½œå‘é‡)
action = model.predict_action(
    **inputs, 
    unnorm_key="bridge_orig",  # æ ¹æ®ä½ çš„æœºå™¨äººé€‰æ‹©
    do_sample=False
)

print(f"é¢„æµ‹åŠ¨ä½œ: {action}")
# è¾“å‡ºç¤ºä¾‹: [0.02, -0.01, 0.05, 0.0, 0.0, 0.1, 1.0]
# [dx, dy, dz, rx, ry, rz, gripper]
```

#### LoRAå¾®è°ƒï¼ˆä½èµ„æºï¼‰

```python
from openvla.training import train_lora

# ä½¿ç”¨LoRAè¿›è¡Œé«˜æ•ˆå¾®è°ƒ
train_lora(
    base_model="openvla/openvla-7b",
    dataset_path="./my_robot_data",
    output_dir="./openvla-finetuned",
    lora_rank=32,
    learning_rate=2e-5,
    num_epochs=10
)
```

---

### æ•™ç¨‹2ï¼šPi0æ¨¡å‹å®æˆ˜

#### ç¯å¢ƒå‡†å¤‡

> **æ³¨æ„**ï¼šPi0æ¨¡å‹å·²äº2025å¹´åˆå¼€æºã€‚è¯·è®¿é—®[å®˜æ–¹GitHubä»“åº“](https://github.com/Physical-Intelligence/openpi)ç¡®è®¤æœ€æ–°å®‰è£…è¯´æ˜ã€‚

```bash
# 1. å…‹éš†ä»“åº“ï¼ˆåŒ…å«å­æ¨¡å—ï¼‰
git clone --recurse-submodules https://github.com/Physical-Intelligence/openpi.git
cd openpi

# 2. ä½¿ç”¨uvåŒ…ç®¡ç†å™¨å®‰è£…ï¼ˆæ¨èï¼‰
pip install uv
uv sync

# æˆ–ä½¿ç”¨ä¼ ç»Ÿpip
python -m venv venv
source venv/bin/activate
pip install -e .
```

#### ä½¿ç”¨LeRoboté›†æˆ

```bash
# å®‰è£…LeRobotä¸Pi0æ”¯æŒ
pip install "lerobot[pi]@git+https://github.com/huggingface/lerobot.git"
```

#### è®­ç»ƒè‡ªå®šä¹‰ä»»åŠ¡

```bash
# ä½¿ç”¨LeRobotè„šæœ¬è¿›è¡Œå¾®è°ƒ
CUDA_VISIBLE_DEVICES=0 python src/lerobot/scripts/train.py \
    --policy.path=lerobot/pi0 \
    --dataset.repo_id=your_username/your_dataset \
    --output_dir=outputs/train/my_pi0 \
    --job_name=pi0_custom \
    --policy.device=cuda \
    --task="pick up the apple and place it in the bowl" \
    --wandb.enable=true
```

#### æ¨ç†ç¤ºä¾‹

```python
from lerobot.common.policies.pi0.policy import PI0Policy
from PIL import Image
import torch

# åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
policy = PI0Policy.from_pretrained("outputs/train/my_pi0/checkpoints/best")
policy = policy.to("cuda")
policy.eval()

# å‡†å¤‡è§‚æµ‹æ•°æ®
observation = {
    "image": torch.from_numpy(camera_image).permute(2, 0, 1).unsqueeze(0).to("cuda"),
    "instruction": "pick up the red cube"
}

# ç”ŸæˆåŠ¨ä½œ
with torch.no_grad():
    action = policy.select_action(observation)

print(f"åŠ¨ä½œè¾“å‡º: {action}")
```

---

### æ•™ç¨‹3ï¼šGemini Robotics-ER APIä½¿ç”¨

Gemini Robotics-ER 1.5å¯é€šè¿‡Google AI Studioå’ŒGemini APIè®¿é—®ã€‚

#### APIè®¾ç½®

```python
import os
import google.generativeai as genai
from PIL import Image
import base64

# é…ç½®APIå¯†é’¥ï¼ˆæ¨èä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
# åœ¨ç»ˆç«¯è®¾ç½®: export GOOGLE_API_KEY="your-api-key"
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# ä½¿ç”¨Gemini Robotics-ERè¿›è¡Œç©ºé—´æ¨ç†
model = genai.GenerativeModel("gemini-1.5-pro-latest")

# åŠ è½½æœºå™¨äººè§†è§’å›¾åƒ
image = Image.open("robot_scene.jpg")

# ç©ºé—´æ¨ç†è¯·æ±‚
response = model.generate_content([
    "ä½œä¸ºä¸€ä¸ªæœºå™¨äººåŠ©æ‰‹ï¼Œåˆ†æè¿™ä¸ªåœºæ™¯å¹¶è§„åˆ’å¦‚ä½•æ•´ç†æ¡Œé¢ä¸Šçš„ç‰©å“ã€‚",
    "è¯·åˆ—å‡ºè¯¦ç»†çš„åŠ¨ä½œæ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤åŒ…æ‹¬ï¼š",
    "1. ç›®æ ‡ç‰©ä½“åŠå…¶ä½ç½®",
    "2. æŠ“å–ç­–ç•¥",
    "3. æ”¾ç½®ç›®æ ‡ä½ç½®",
    image
])

print(response.text)
```

#### è§„åˆ’ä¸æ‰§è¡Œç¤ºä¾‹

```python
# ä½¿ç”¨Geminiè¿›è¡Œä»»åŠ¡è§„åˆ’
planning_prompt = """
ä½ æ˜¯ä¸€ä¸ªæœºå™¨äººä»»åŠ¡è§„åˆ’å™¨ã€‚æ ¹æ®ä»¥ä¸‹åœºæ™¯æè¿°ï¼Œç”Ÿæˆä¸€ä¸ªç»“æ„åŒ–çš„åŠ¨ä½œè®¡åˆ’ï¼š

åœºæ™¯ï¼šå¨æˆ¿å°é¢ä¸Šæœ‰3ä¸ªç‰©å“ - ä¸€ä¸ªçº¢è‰²æ¯å­ã€ä¸€ä¸ªè“è‰²ç¢—ã€ä¸€ä¸ªç»¿è‰²ç›˜å­ã€‚
ä»»åŠ¡ï¼šå°†æ¯å­æ”¾å…¥ç¢—ä¸­ã€‚

è¯·è¾“å‡ºJSONæ ¼å¼çš„åŠ¨ä½œåºåˆ—ã€‚
"""

response = model.generate_content(planning_prompt)
action_plan = json.loads(response.text)

# åŠ¨ä½œè®¡åˆ’ç¤ºä¾‹è¾“å‡ºï¼š
# {
#   "steps": [
#     {"action": "approach", "target": "red_cup", "position": [0.2, 0.1, 0.15]},
#     {"action": "grasp", "gripper_width": 0.06},
#     {"action": "lift", "height": 0.1},
#     {"action": "move_to", "target": "blue_bowl", "position": [0.3, -0.1, 0.2]},
#     {"action": "place", "gripper_width": 0.08}
#   ]
# }
```

---

## VLAæ¨¡å‹é¢ä¸´çš„åå¤§æŒ‘æˆ˜

æ ¹æ®æœ€æ–°çš„[ç ”ç©¶ç»¼è¿°ï¼ˆarXiv:2511.05936ï¼‰](https://arxiv.org/html/2511.05936v1)ï¼ŒVLAæ¨¡å‹å‘å±•é¢ä¸´ä»¥ä¸‹æ ¸å¿ƒæŒ‘æˆ˜ï¼š

```mermaid
mindmap
  root((VLAæŒ‘æˆ˜))
    å¤šæ¨¡æ€æ„ŸçŸ¥
      è§†è§‰-è§¦è§‰èåˆ
      éŸ³é¢‘ç†è§£
      æœ¬ä½“æ„ŸçŸ¥
    æ¨ç†èƒ½åŠ›
      ç©ºé—´æ¨ç†
      å› æœæ¨ç†
      å¸¸è¯†æ¨ç†
    æ•°æ®é—®é¢˜
      çœŸå®æ•°æ®ç¨€ç¼º
      åˆæˆæ•°æ®è´¨é‡
      è·¨åŸŸè¿ç§»
    è¯„ä¼°æ ‡å‡†
      åŸºå‡†ç»Ÿä¸€
      å®‰å…¨æ€§è¯„ä¼°
      é•¿æœŸæ€§èƒ½
    æ³›åŒ–èƒ½åŠ›
      æ–°ç‰©ä½“é€‚åº”
      æ–°ç¯å¢ƒé€‚åº”
      è·¨æœºå™¨äººè¿ç§»
    æ•ˆç‡ä¼˜åŒ–
      å®æ—¶æ¨ç†
      è¾¹ç¼˜éƒ¨ç½²
      èƒ½è€—ä¼˜åŒ–
    å…¨èº«åè°ƒ
      åŒè‡‚åä½œ
      ç§»åŠ¨æ“ä½œ
      ç²¾ç»†æ§åˆ¶
    å®‰å…¨æ€§
      ç¢°æ’é¿å…
      åŠ›æ§åˆ¶
      ä¼¦ç†çº¦æŸ
    å¤šæ™ºèƒ½ä½“
      åä½œè§„åˆ’
      ä»»åŠ¡åˆ†é…
      é€šä¿¡æ•ˆç‡
    äººæœºåä½œ
      æ„å›¾ç†è§£
      è‡ªç„¶äº¤äº’
      ä¿¡ä»»å»ºç«‹
```

### é‡ç‚¹æŒ‘æˆ˜è§£æ

1. **å¤šæ¨¡æ€èåˆ**ï¼šå½“å‰æ¨¡å‹ä¸»è¦ä¾èµ–è§†è§‰ï¼Œè§¦è§‰ã€åŠ›åé¦ˆç­‰æ¨¡æ€çš„é›†æˆä»ä¸æˆç†Ÿ

2. **Sim2Realå·®è·**ï¼šä»¿çœŸè®­ç»ƒçš„ç­–ç•¥éš¾ä»¥ç›´æ¥è¿ç§»åˆ°çœŸå®æœºå™¨äºº

3. **å®æ—¶æ€§è¦æ±‚**ï¼šVLAæ¨¡å‹æ¨ç†å»¶è¿Ÿï¼ˆ100ms+ï¼‰éš¾ä»¥æ»¡è¶³é«˜é€Ÿæ“ä½œéœ€æ±‚

4. **é•¿horizonä»»åŠ¡**ï¼šå¤šæ­¥éª¤ä»»åŠ¡çš„è§„åˆ’å’Œæ‰§è¡Œä»æ˜¯éš¾é¢˜

---

## æ€»ç»“ä¸å±•æœ›

### VLAæŠ€æœ¯å‘å±•è¶‹åŠ¿

```mermaid
timeline
    title VLAæŠ€æœ¯å‘å±•è·¯çº¿å›¾
    2023 : RT-2å‘å¸ƒ
         : VLAæ¦‚å¿µç¡®ç«‹
    2024 : OpenVLAå¼€æº
         : Pi0å‘å¸ƒ
         : Octoå¼€æº
    2025 : Gemini Robotics 1.5
         : Figure Helix
         : å¤šæ¨¡æ€èåˆ
    2026 : è§¦è§‰VLAæ™®åŠ
         : å®æ—¶è¾¹ç¼˜éƒ¨ç½²
         : äººå½¢æœºå™¨äººå•†ç”¨
```

### å…³é”®å»ºè®®

**å¯¹äºç ”ç©¶è€…**ï¼š
- ä¼˜å…ˆå°è¯•OpenVLAå’ŒPi0ç­‰å¼€æºæ¨¡å‹
- å…³æ³¨å¤šæ¨¡æ€èåˆå’ŒSim2Realè¿ç§»ç ”ç©¶
- å‚ä¸Open X-Embodimentæ•°æ®é›†è´¡çŒ®

**å¯¹äºå¼€å‘è€…**ï¼š
- ä»OpenVLAçš„LoRAå¾®è°ƒå¼€å§‹
- åˆ©ç”¨Genesisç­‰ä»¿çœŸå¹³å°è¿›è¡Œå¿«é€Ÿè¿­ä»£
- å…³æ³¨è¾¹ç¼˜éƒ¨ç½²ä¼˜åŒ–ï¼ˆå¦‚NVIDIA Jetsonï¼‰

**å¯¹äºä¼ä¸š**ï¼š
- è¯„ä¼°Gemini Robotics APIçš„å•†ä¸šåº”ç”¨
- å…³æ³¨Figureå’ŒPhysical Intelligenceçš„äº§å“åŒ–è¿›å±•
- å»ºç«‹æœºå™¨äººæ•°æ®é‡‡é›†èƒ½åŠ›

### èµ„æºæ±‡æ€»

| èµ„æºç±»å‹ | é“¾æ¥ |
|---------|------|
| OpenVLA GitHub | [github.com/openvla/openvla](https://github.com/openvla/openvla) |
| Pi0 (openpi) GitHub | [github.com/Physical-Intelligence/openpi](https://github.com/Physical-Intelligence/openpi) |
| Gemini APIæ–‡æ¡£ | [ai.google.dev/gemini-api](https://ai.google.dev/gemini-api) |
| Figure Helixä»‹ç» | [figure.ai/news/helix](https://www.figure.ai/news/helix) |
| VLAæŒ‘æˆ˜ç»¼è¿° | [arxiv.org/abs/2511.05936](https://arxiv.org/abs/2511.05936) |
| Awesome VLA Robotics | [github.com/Jiaaqiliu/Awesome-VLA-Robotics](https://github.com/Jiaaqiliu/Awesome-VLA-Robotics) |

---

## å‚è€ƒæ–‡çŒ®

1. Kim, M., et al. "OpenVLA: An Open-Source Vision-Language-Action Model." arXiv:2406.09246, 2024.
2. Google DeepMind. "Gemini Robotics 1.5 brings AI agents into the physical world." 2025.
3. Figure AI. "Helix: A Vision-Language-Action Model for Generalist Humanoid Control." 2025.
4. Physical Intelligence. "Ï€â‚€: A Vision-Language-Action Flow Model for General Robot Control." 2024.
5. Team OXE. "Open X-Embodiment: Robotic Learning Datasets and RT-X Models." 2024.
6. "10 Open Challenges Steering the Future of Vision-Language-Action Models." arXiv:2511.05936, 2025.

---

*æœ¬æ–‡æ’°å†™äº2025å¹´12æœˆ6æ—¥ï¼ŒåŸºäºæœ€æ–°å…¬å¼€çš„ç ”ç©¶è®ºæ–‡å’Œå®˜æ–¹å‘å¸ƒä¿¡æ¯ã€‚éšç€VLAé¢†åŸŸçš„å¿«é€Ÿå‘å±•ï¼Œå»ºè®®æŒç»­å…³æ³¨å„é¡¹ç›®çš„å®˜æ–¹æ›´æ–°ã€‚*

**è”ç³»ä¸è®¨è®º**ï¼š
- æ¬¢è¿åœ¨GitHub Issuesä¸­æé—®æŠ€æœ¯é—®é¢˜
- åŠ å…¥HuggingFaceç¤¾åŒºè®¨è®ºVLAç ”ç©¶è¿›å±•
- å…³æ³¨å„å¼€æºé¡¹ç›®çš„Discord/Slacké¢‘é“è·å–æœ€æ–°åŠ¨æ€
